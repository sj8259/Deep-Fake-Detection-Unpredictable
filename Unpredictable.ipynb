{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2bd_MhvyI2A",
        "outputId": "6216d4da-0e05-4673-fe04-f26fc2c9f312"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hnjq1mgEzhIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = '/content/drive/My Drive/DATASET.zip'  # Replace with your file's path\n",
        "extract_path = '/content/dataset'\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "os.listdir(extract_path)  # Check the extracted files\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1_G43PSy1At",
        "outputId": "97568934-72a8-4d8a-fc12-457aef05dba0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['DATASET', '__MACOSX']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import json\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.applications import InceptionResNetV2\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n"
      ],
      "metadata": {
        "id": "iIm42S1e2otv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Paths to dataset folders\n",
        "base_path = '/content/dataset/DATASET'\n",
        "fake_images_path = os.path.join(base_path, 'fake_cifake_images')\n",
        "real_images_path = os.path.join(base_path, 'real_cifake_images')\n",
        "\n",
        "# Load the JSON files for real and fake images\n",
        "fake_json_path = os.path.join(base_path, 'fake_cifake_preds.json')\n",
        "real_json_path = os.path.join(base_path, 'real_cifake_preds.json')\n",
        "\n",
        "# Load fake image JSON data\n",
        "with open(fake_json_path, 'r') as f:\n",
        "    fake_data = json.load(f)\n",
        "\n",
        "# Load real image JSON data\n",
        "with open(real_json_path, 'r') as f:\n",
        "    real_data = json.load(f)\n",
        "\n",
        "# Function to load images from directory and preprocess\n",
        "def load_and_preprocess_images(image_folder, json_data, image_size=(224, 224)):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for entry in json_data:\n",
        "        # Image filename corresponds to the index in the JSON\n",
        "        image_filename = f\"{entry['index']}.png\"\n",
        "        image_path = os.path.join(image_folder, image_filename)\n",
        "\n",
        "        # Read and preprocess the image (resize and normalize)\n",
        "        img = cv2.imread(image_path)\n",
        "        if img is None:\n",
        "            continue  # Skip if image is missing or unreadable\n",
        "        img = cv2.resize(img, image_size)  # Resize to a fixed size (e.g., 224x224)\n",
        "        img = img.astype('float32') / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "        images.append(img)\n",
        "        labels.append(0 if entry['prediction'] == 'fake' else 1)  # 0 for fake, 1 for real\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Load and preprocess fake and real images\n",
        "fake_images, fake_labels = load_and_preprocess_images(fake_images_path, fake_data)\n",
        "real_images, real_labels = load_and_preprocess_images(real_images_path, real_data)\n",
        "\n",
        "# Combine the datasets (fake and real images)\n",
        "X = np.concatenate([fake_images, real_images], axis=0)\n",
        "y = np.concatenate([fake_labels, real_labels], axis=0)\n",
        "\n",
        "# Shuffle and split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Testing data shape: {X_test.shape}\")\n",
        "print(f\"Training labels shape: {y_train.shape}\")\n",
        "print(f\"Testing labels shape: {y_test.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4jjwSNJ2q7o",
        "outputId": "b73842e1-1212-4e61-9e28-0a334e0a79d2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (1600, 224, 224, 3)\n",
            "Testing data shape: (400, 224, 224, 3)\n",
            "Training labels shape: (1600,)\n",
            "Testing labels shape: (400,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess test images\n",
        "def load_test_images(test_image_folder, image_size=(224, 224)):\n",
        "    test_images = []\n",
        "\n",
        "    for image_name in os.listdir(test_image_folder):\n",
        "        image_path = os.path.join(test_image_folder, image_name)\n",
        "\n",
        "        # Read and preprocess the image (resize and normalize)\n",
        "        img = cv2.imread(image_path)\n",
        "        img = cv2.resize(img, image_size)  # Resize to a fixed size (e.g., 224x224)\n",
        "        img = img.astype('float32') / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "        test_images.append(img)\n",
        "\n",
        "    return np.array(test_images)\n",
        "\n",
        "# Load and preprocess test images\n",
        "test_images = load_test_images('/content/dataset/DATASET/test')\n",
        "print(f\"Test data shape: {test_images.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PeQ1Eyj3Wkp",
        "outputId": "45cc6076-5981-4aa6-d2e1-a3366b55df3b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test data shape: (500, 224, 224, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import InceptionResNetV2\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Initialize InceptionResNetV2 model with pre-trained weights (no top classification layer)\n",
        "base_model = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the base model layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# Build the model\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(1024, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Output: 1 neuron (fake or real)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "VkoYWQFj3cmY",
        "outputId": "a01ae801-a67b-4151-d882-0c2bc2379aa2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m219055592/219055592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ inception_resnet_v2 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m1536\u001b[0m)          │      \u001b[38;5;34m54,336,736\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling2d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1536\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                │       \u001b[38;5;34m1,573,888\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │           \u001b[38;5;34m1,025\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ inception_resnet_v2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)          │      <span style=\"color: #00af00; text-decoration-color: #00af00\">54,336,736</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling2d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,573,888</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,025</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m55,911,649\u001b[0m (213.29 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">55,911,649</span> (213.29 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,574,913\u001b[0m (6.01 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,913</span> (6.01 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m54,336,736\u001b[0m (207.28 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">54,336,736</span> (207.28 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model with manual validation data\n",
        "history = model.fit(\n",
        "    datagen.flow(X_train, y_train, batch_size=32),\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=10\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpmDya3c3_oS",
        "outputId": "a2cade28-8952-4d26-d948-1a00b7d2f735"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 2s/step - accuracy: 0.5638 - loss: 1.7768 - val_accuracy: 0.6537 - val_loss: 0.5829\n",
            "Epoch 2/10\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 447ms/step - accuracy: 0.7258 - loss: 0.5706 - val_accuracy: 0.7317 - val_loss: 0.5256\n",
            "Epoch 3/10\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 434ms/step - accuracy: 0.7481 - loss: 0.5204 - val_accuracy: 0.7463 - val_loss: 0.5057\n",
            "Epoch 4/10\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 424ms/step - accuracy: 0.7803 - loss: 0.4425 - val_accuracy: 0.7707 - val_loss: 0.4903\n",
            "Epoch 5/10\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 402ms/step - accuracy: 0.8180 - loss: 0.4097 - val_accuracy: 0.7951 - val_loss: 0.4232\n",
            "Epoch 6/10\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 453ms/step - accuracy: 0.8187 - loss: 0.4020 - val_accuracy: 0.8195 - val_loss: 0.4313\n",
            "Epoch 7/10\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 472ms/step - accuracy: 0.8104 - loss: 0.4395 - val_accuracy: 0.7805 - val_loss: 0.4495\n",
            "Epoch 8/10\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 462ms/step - accuracy: 0.8100 - loss: 0.4232 - val_accuracy: 0.7951 - val_loss: 0.4246\n",
            "Epoch 9/10\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 410ms/step - accuracy: 0.8467 - loss: 0.3677 - val_accuracy: 0.8000 - val_loss: 0.4544\n",
            "Epoch 10/10\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 467ms/step - accuracy: 0.8150 - loss: 0.3951 - val_accuracy: 0.7854 - val_loss: 0.4835\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Initialize ImageDataGenerator for data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,       # Randomly rotate images in the range (degrees)\n",
        "    width_shift_range=0.2,   # Randomly shift images horizontally\n",
        "    height_shift_range=0.2,  # Randomly shift images vertically\n",
        "    shear_range=0.2,         # Random shear transformation\n",
        "    zoom_range=0.2,          # Random zoom\n",
        "    horizontal_flip=True,    # Randomly flip images horizontally\n",
        "    fill_mode='nearest'      # Fill mode for empty pixels\n",
        ")\n",
        "\n",
        "# Since we already split X_train and y_train previously,\n",
        "# further split the training set into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
        ")\n",
        "\n",
        "# Train the model with manual validation\n",
        "history = model.fit(\n",
        "    datagen.flow(X_train, y_train, batch_size=32),  # Apply augmentation to training set\n",
        "    validation_data=(X_val, y_val),  # No augmentation for validation set\n",
        "    epochs=20,  # Specify the number of epochs\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# Evaluate model performance on the test set (which was untouched during training)\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Final Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Confirm test set shape\n",
        "print(f\"Final Test Data Shape: {X_test.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8fO16YD4Si8",
        "outputId": "d24ef227-8d85-479d-e99d-6071e3880d7a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2s/step - accuracy: 0.8651 - loss: 0.3264 - val_accuracy: 0.8855 - val_loss: 0.2742\n",
            "Epoch 2/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 484ms/step - accuracy: 0.9162 - loss: 0.2186 - val_accuracy: 0.9389 - val_loss: 0.2277\n",
            "Epoch 3/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 381ms/step - accuracy: 0.9189 - loss: 0.2101 - val_accuracy: 0.9008 - val_loss: 0.2343\n",
            "Epoch 4/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 374ms/step - accuracy: 0.8756 - loss: 0.2775 - val_accuracy: 0.9237 - val_loss: 0.2254\n",
            "Epoch 5/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 522ms/step - accuracy: 0.8718 - loss: 0.2999 - val_accuracy: 0.9008 - val_loss: 0.2329\n",
            "Epoch 6/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 424ms/step - accuracy: 0.8803 - loss: 0.2747 - val_accuracy: 0.9237 - val_loss: 0.2626\n",
            "Epoch 7/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 479ms/step - accuracy: 0.9279 - loss: 0.2002 - val_accuracy: 0.8931 - val_loss: 0.2610\n",
            "Epoch 8/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 423ms/step - accuracy: 0.9098 - loss: 0.2477 - val_accuracy: 0.8779 - val_loss: 0.2773\n",
            "Epoch 9/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 464ms/step - accuracy: 0.9005 - loss: 0.2547 - val_accuracy: 0.8550 - val_loss: 0.3227\n",
            "Epoch 10/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 378ms/step - accuracy: 0.8827 - loss: 0.2679 - val_accuracy: 0.8397 - val_loss: 0.3260\n",
            "Epoch 11/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 453ms/step - accuracy: 0.8750 - loss: 0.2744 - val_accuracy: 0.8550 - val_loss: 0.3108\n",
            "Epoch 12/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 415ms/step - accuracy: 0.9080 - loss: 0.2459 - val_accuracy: 0.8779 - val_loss: 0.2910\n",
            "Epoch 13/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 379ms/step - accuracy: 0.8553 - loss: 0.3146 - val_accuracy: 0.8550 - val_loss: 0.2994\n",
            "Epoch 14/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 466ms/step - accuracy: 0.8899 - loss: 0.2492 - val_accuracy: 0.8473 - val_loss: 0.3012\n",
            "Epoch 15/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 382ms/step - accuracy: 0.9174 - loss: 0.2322 - val_accuracy: 0.7710 - val_loss: 0.5621\n",
            "Epoch 16/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 471ms/step - accuracy: 0.8673 - loss: 0.3069 - val_accuracy: 0.8702 - val_loss: 0.3046\n",
            "Epoch 17/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 383ms/step - accuracy: 0.9047 - loss: 0.2539 - val_accuracy: 0.8550 - val_loss: 0.3479\n",
            "Epoch 18/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 480ms/step - accuracy: 0.9102 - loss: 0.2161 - val_accuracy: 0.8092 - val_loss: 0.3843\n",
            "Epoch 19/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 418ms/step - accuracy: 0.9250 - loss: 0.2063 - val_accuracy: 0.8321 - val_loss: 0.3985\n",
            "Epoch 20/20\n",
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 460ms/step - accuracy: 0.9406 - loss: 0.1790 - val_accuracy: 0.8626 - val_loss: 0.3146\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 140ms/step - accuracy: 0.8016 - loss: 0.4724\n",
            "Final Test Accuracy: 0.8200\n",
            "Final Test Data Shape: (400, 224, 224, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the test images\n",
        "test_predictions = model.predict(test_images)\n",
        "test_predictions = (test_predictions > 0.5).astype(int)  # Convert to 0 or 1\n",
        "\n",
        "# Create the output JSON in the required format\n",
        "output_data = []\n",
        "for i, pred in enumerate(test_predictions):\n",
        "    output_data.append({\n",
        "        \"index\": i + 1,  # Ensure the index starts from 1\n",
        "        \"prediction\": \"real\" if pred == 1 else \"fake\"\n",
        "    })\n",
        "\n",
        "# Save the predictions to a JSON file\n",
        "output_json_path = 'Unpredictable.json'\n",
        "with open(output_json_path, 'w') as f:\n",
        "    json.dump(output_data, f, indent=4)\n",
        "\n",
        "print(f\"Predictions saved to {output_json_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSvSxQ_g33bo",
        "outputId": "4245f8d0-c332-4781-c635-cb527ed99b8f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step\n",
            "Predictions saved to Unpredictable.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Path to your test images folder\n",
        "test_folder = '/content/dataset/DATASET/test'\n",
        "\n",
        "# List all the image files in the test folder\n",
        "test_image_files = [f for f in os.listdir(test_folder) if f.endswith('.jpg') or f.endswith('.png')]\n",
        "\n",
        "# Prepare the test images and labels (you may have labels for the test set, or it could be unlabeled)\n",
        "test_images = []\n",
        "for img_file in test_image_files:\n",
        "    img_path = os.path.join(test_folder, img_file)\n",
        "    img = image.load_img(img_path, target_size=(224, 224))  # Resizing to 224x224\n",
        "    img_array = image.img_to_array(img)  # Convert to NumPy array\n",
        "    test_images.append(img_array)\n",
        "\n",
        "# Convert the list to a NumPy array\n",
        "test_images = np.array(test_images)\n",
        "\n",
        "# Normalize the images\n",
        "test_images = test_images / 255.0  # Ensure the images are scaled between 0 and 1\n"
      ],
      "metadata": {
        "id": "2feeFajQ7Wl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(test_images)\n",
        "\n",
        "# Convert predictions to binary labels (if necessary, e.g., 0 for fake, 1 for real)\n",
        "predicted_labels = (predictions > 0.5).astype(\"int32\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZrzW4E57osr",
        "outputId": "079b90bc-c08c-4278-f142-cf03c5365549"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 141ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set (500 test images)\n",
        "predictions = model.predict(test_images)\n",
        "\n",
        "# Since the model output is likely a probability (e.g., for binary classification),\n",
        "# you can convert these to binary labels (0 or 1) by setting a threshold (usually 0.5).\n",
        "predicted_labels = (predictions > 0.5).astype(int)\n",
        "\n",
        "# Check the shape and a few predictions\n",
        "print(f\"Predicted labels shape: {predicted_labels.shape}\")\n",
        "print(f\"First 10 predicted labels: {predicted_labels[:10]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O31YDA0t8Wx-",
        "outputId": "fb1eadb3-2de9-4173-903f-b1718b539fed"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 143ms/step\n",
            "Predicted labels shape: (500, 1)\n",
            "First 10 predicted labels: [[1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the training and validation data (if available)\n",
        "train_loss, train_accuracy = model.evaluate(X_train, y_train)\n",
        "val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
        "\n",
        "print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
        "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziCYfijd9lN_",
        "outputId": "db361feb-37bb-4bda-b637-2bfd56812626"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 505ms/step - accuracy: 0.9610 - loss: 0.1331\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 122ms/step - accuracy: 0.8730 - loss: 0.2819\n",
            "Training Accuracy: 95.80%\n",
            "Validation Accuracy: 86.26%\n"
          ]
        }
      ]
    }
  ]
}